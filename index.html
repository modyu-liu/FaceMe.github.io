<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>FaceMe</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
</head>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Center GIF</title>
  <style>
    .content {
      text-align: center; /* 水平居中 */
    }
    .content img {
      display: inline-block; /* 保持图片为行内块元素 */
    }
  </style>
</head>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Interactive Image Comparison</title>
  <style>
      /* 样式调整 */
      .columns {
          display: flex;
          flex-wrap: wrap; /* 允许多列换行 */
          justify-content: center;
      }
      .column {
          flex-basis: 25%; /* 每个图像占父容器宽度的 25% */
          max-width: 25%; /* 最大宽度也是 25% */
          box-sizing: border-box; /* 包含内边距和边框 */
          padding: 10px; /* 图像之间的间距 */
      }
      .img-comp-container {
          position: relative;
          width: auto;
          height: auto;
          margin-bottom: 10px;
      }
      .img-comp-img img {
          display: block;
          width: 100%;
          height: auto;
      }
      .img-comp-overlay {
          position: absolute;
          top: 0;
          left: 0;
          height: 100%;
          width: 50%; /* 初始覆盖 50% */
          overflow: hidden;
      }
      .img-comp-overlay img {
          position: absolute;
          top: 0;
          left: 0;
          width: auto;
          height: 100%;
      }
      .img-comp-slider {
          position: absolute;
          z-index: 9;
          cursor: ew-resize;
          background-color: #000;
          height: 100%;
          width: 5px;
          top: 0;
          left: 50%; /* 初始在中间 */
      }
  </style>
</head>

<head>
  <meta charset="UTF-8">
  <title>MathJax Example</title>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="content">
  <h1><strong>FaceMe: Robust Blind Face Restoration With Personal Identification</strong></h1>
  
  <p id="authors">
    <a href="">Siyu Liu<sup>1,*</sup></a>, 
    <a href="">Zhengpeng Duan<sup>1,*</sup></a>, 
    <a href="">Jia OuYang<sup>2</sup></a>, 
    <a href="">Jiayi Fu<sup>1</sup></a>,<br>
    <a href="">Hyunhee Park<sup>3</sup></a>, 
    <a href="">Zikun Liu<sup>2</sup></a>,
    <a href="">Chunle Guo<sup>1,&dagger;</sup></a>,
    <a href="">Chongyi Li<sup>1</sup></a>,
    <br><br>
    <span style="font-size: 16px; font-weight: bold;"><sup>1</sup>VCIP, CS, Nankai University</span> <br>
    <span style="font-size: 16px; font-weight: bold;"><sup>2</sup>Samsung Research, China, Beijing (SRC-B)</span> <br>
    <span style="font-size: 16px; font-weight: bold;"><sup>3</sup>The Department of Camera Innovation Group, Samsung Electronics</span> <br>
    <span style="font-size: 16px; font-weight: bold;"><sup>*</sup>Equal contribution. <sup>&dagger;</sup>Corresponding author.</span> <br>
    
  </p>
  <br>
  
  <figure>
    <img src="./FaceMe_files/teaser.png" alt="Teaser image" style="width: 100%;">
  </figure>
  
  <h3 style="text-align: center; font-style: italic;">
    Make the people in the photo look like you and those you are familiar with. Using a single or a few reference images,
    we can restore realistic images without any fine-tuning for identity. 
  </h3>
  
  <img src="FaceMe_files/demo.gif" alt="Example GIF">
  <h3 style="text-align: center; font-style: italic;">
    FaceMe can effectively maintain identity consistency as
    degradation level increases, while also balancing the dependency between the low-quality input and the reference image.
  </h3>

  <p style="text-align: center; font-size: 1.5em;">
      <a href="" target="_blank">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank">[Dataset]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank">[BibTeX]</a>
  </p>

</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and supports any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness. </p>
</div>
<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/background.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2 style="text-align:center;">Method</h2>
  <p>FaceMe is a fine-tuning-free personalized blind face restoration method based on diffusion model. Given a low-quality input and either single or a few high-quality references images of the same identity, FaceMe restores high-quality facial images and maintains identity consistency within seconds. </p>
  <br>
  <figure>
    <img src="./FaceMe_files/overview.png" alt="Overview image" style="width: 100%;">
  </figure>
  <p>
    For the proposed FaceMe, identity-related features from the reference image are extracted by an identity encoder, 
    simply combined to support multi-reference image inputs. We use a fixed text, 
    <i>i.e., "a photo of a face."</i>, and the combined identity-related features replace the embedding corresponding to 
    <i>"face."</i> The updated embeddings are sent to the cross-attention layer of the diffusion model to guide personalized face image restoration.
  </p>
  <p>
    To address the challenge of insufficient datasets for personalized blind face restoration, we utilize synthetic facial images as reference images to build our training data pool.
    We synthesize multiple reference facial images of the same identity as the given facial image using Arc2Face (Papantoniou et al. 2024), equipped with ControlNet (Zhang,Rao, and Agrawala 2023). 
    Given a pair of facial images (\(x_{ref} , x_{pose}\)), Arc2Face can synthesize facial images that maintain the identity of \(x_{ref}\) and the pose of \(x_{pose}\).
    We divide the FFHQ dataset into several subsets based on expressions and poses. 
    For each image synthesis, a facial image is randomly selected from each subset to serve as the pose reference \(x_{pose}\).
  </p>
</div>

<div class="content">
  <h2 style="text-align:center;">Results</h2>
  <h2>Quantitative Comparisons</h2>
  <figure>
    <img src="./FaceMe_files/Quantitative_Comparisons.png" alt="Quantitative_Comparisons" style="width: 100%;">
  </figure>
  <p>
  FaceMe achieves the best performance in PSNR, FID, LMD, and IDS, and the second-best performance in LPIPS. 
  It is worth noting that FaceMe has significantly improved in both LMD and IDS, which demonstrates its ability in personalization while minimizes the impact of ID-irrelevant features. 
  </p>
  <h2>Visual Comparisons</h2>
  <figure>
    <img src="./FaceMe_files/synth_result.png" alt="synth_result" style="width: 100%;">
  </figure>
  <figure>
    <img src="./FaceMe_files/real_result.png" alt="real_result" style="width: 100%;">
  </figure>
</div>

<div class="content">
  <h2 style="text-align:center;">Interactive Comparison</h2>
  <div class="columns" id="image-container"></div>

    <script>
        // 图片路径数据：数组包含输入和输出图片的路径
        const images = [
            { input: "./FaceMe_files/compare_result/lq/Danny_Green_0001.jpg", output: "./FaceMe_files/compare_result/sr/Danny_Green_0001.png" },
            { input: "./FaceMe_files/compare_result/lq/Svend_Aage_Jensby_0001.jpg", output: "./FaceMe_files/compare_result/sr/Svend_Aage_Jensby_0001.png" },
            { input: "./FaceMe_files/compare_result/lq/Vaclav_Havel_0001.jpg", output: "./FaceMe_files/compare_result/sr/Vaclav_Havel_0001.png" },
            { input: "./FaceMe_files/compare_result/lq/Yasushi_Chimura_0001.jpg", output: "./FaceMe_files/compare_result/sr/Yasushi_Chimura_0001.png" },
            { input: "./FaceMe_files/compare_result/lq/00016_00.png", output: "./FaceMe_files/compare_result/sr/00016_00.png" },
            { input: "./FaceMe_files/compare_result/lq/00101_00.png", output: "./FaceMe_files/compare_result/sr/00101_00.png" },
            { input: "./FaceMe_files/compare_result/lq/10008_02.png", output: "./FaceMe_files/compare_result/sr/10008_02.png" },
            { input: "./FaceMe_files/compare_result/lq/10009_02.png", output: "./FaceMe_files/compare_result/sr/10009_02.png" },
            { input: "./FaceMe_files/compare_result/lq/0761.png", output: "./FaceMe_files/compare_result/sr/0761.png" },
            { input: "./FaceMe_files/compare_result/lq/0032.png", output: "./FaceMe_files/compare_result/sr/0032.png" },
            { input: "./FaceMe_files/compare_result/lq/0909.png", output: "./FaceMe_files/compare_result/sr/0909.png" },
            { input: "./FaceMe_files/compare_result/lq/0285.png", output: "./FaceMe_files/compare_result/sr/0285.png" },
        
          ];

        // 获取父容器
        const container = document.getElementById("image-container");

        // 动态生成图片结构
        images.forEach(pair => {
            const column = document.createElement("div");
            column.className = "column";

            column.innerHTML = `
                <div class="img-comp-container">
                    <div class="img-comp-img">
                        <img src="${pair.input}" alt="Input Image">
                    </div>
                    <div class="img-comp-img img-comp-overlay">
                        <img src="${pair.output}" alt="Output Image">
                    </div>
                    <div class="img-comp-slider"></div>
                </div>
            `;
            container.appendChild(column);
        });

        // 添加交互功能
        const initComparisons = () => {
            const containers = document.querySelectorAll('.img-comp-container');
            containers.forEach(container => {
                const overlay = container.querySelector('.img-comp-overlay');
                const slider = container.querySelector('.img-comp-slider');

                // 初始化变量
                let isDragging = false;

                // 鼠标事件
                slider.addEventListener('mousedown', startDrag);
                window.addEventListener('mousemove', drag);
                window.addEventListener('mouseup', endDrag);

                // 移动端触控事件
                slider.addEventListener('touchstart', startDrag);
                window.addEventListener('touchmove', drag);
                window.addEventListener('touchend', endDrag);

                function startDrag(e) {
                    isDragging = true;
                    drag(e);
                }

                function drag(e) {
                    if (!isDragging) return;

                    const rect = container.getBoundingClientRect();
                    const xPos = e.touches ? e.touches[0].clientX - rect.left : e.clientX - rect.left;

                    // 限制滑块范围
                    const width = container.offsetWidth;
                    const sliderPos = Math.max(0, Math.min(xPos, width));

                    // 更新覆盖层和滑块的位置
                    overlay.style.width = sliderPos + "px";
                    slider.style.left = sliderPos + "px";
                }

                function endDrag() {
                    isDragging = false;
                }
            });
        };

        // 初始化所有图片的对比功能
        initComparisons();
    </script>
<!--     
  <div class="columns is-centered">
    <div class="column">
      <div class="img-comp-container" style="margin-bottom: 10px;">
        <div class="img-comp-img">
          <img src="static/imgs/1_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/1_ours.png" width="224" height="224">
        </div>
      </div>
      <div class="img-comp-container">
        <div class="img-comp-img">
          <img src="static/imgs/3_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/3_ours.png" width="224" height="224">
        </div>
      </div>
    </div>

    <div class="column">
      <div class="img-comp-container" style="margin-bottom: 10px;">
        <div class="img-comp-img">
          <img src="static/imgs/2_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/2_ours.png" width="224" height="224">
        </div>
      </div>
      <div class="img-comp-container">
        <div class="img-comp-img">
          <img src="static/imgs/4_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/4_ours.png" width="224" height="224">
        </div>
      </div>
    </div>

    <div class="column">
      <div class="img-comp-container" style="margin-bottom: 10px;">
        <div class="img-comp-img">
          <img src="static/imgs/f1_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/f1_ours.png" width="224" height="224">
        </div>
      </div>
      <div class="img-comp-container">
        <div class="img-comp-img">
          <img src="static/imgs/l1_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/l1_ours.png" width="224" height="224">
        </div>
      </div>
    </div>

    <div class="column" >
      <div class="img-comp-container" style="margin-bottom: 10px;">
        <div class="img-comp-img">
          <img src="static/imgs/f2_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/f2_ours.png" width="224" height="224">
        </div>
      </div>
      <div class="img-comp-container">
        <div class="img-comp-img">
          <img src="static/imgs/p2_input.png" width="224" height="224">
        </div>
        <div class="img-comp-img img-comp-overlay">
          <img src="static/imgs/p2_ours.png" width="224" height="224">
        </div>
      </div>
    </div>

  </div> -->

  <p>
    You can slide the images to clearly observe the differences between the input on the right and the output on the left. 
    The first row is from the LFW-Test; the second row is from the WebPhoto-Test; 
    and the third row is from the Wider-Test. 
    FaceMe can restore high-fidelity and high-quality images, while previous methods produce the unrealistic artifacts.
  </p>
</div>

<div class="content">
  <h2>BibTex</h2>
  <!-- <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code>  -->
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
  </p>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <!-- <div class="column is-8"> -->
          <p>
            This website's source code is borrowed from the <a
            href="https://github.com/google/dreambooth">DreamBooth</a> project page.
          </p>
      <!-- </div> -->
    </div>
  </div>
</footer>

</body>
</html>
