<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>FaceMe</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
</head>

<head>
  <meta charset="UTF-8">
  <title>MathJax Example</title>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="content">
  <h1><strong>FaceMe: Robust Blind Face Restoration With Personal Identification</strong></h1>
  
  <p id="authors">
    <a href="">Siyu Liu<sup>1</sup></a>, 
    <a href="">Zhengpeng Duan<sup>1</sup></a>, 
    <a href="">Jia OuYang<sup>2</sup></a>, 
    <a href="">Jiayi Fu<sup>1</sup></a>,<br>
    <a href="">Hyunhee Park<sup>3</sup></a>, 
    <a href="">Zikun Liu<sup>2</sup></a>,
    <a href="">Chunle Guo<sup>1</sup></a>,
    <a href="">Chongyi Li<sup>1</sup></a>,
    <br><br>
    <span style="font-size: 16px; font-weight: bold;"><sup>1</sup>VCIP, CS, Nankai University</span> <br>
    <span style="font-size: 16px; font-weight: bold;"><sup>2</sup>Samsung Research, China, Beijing (SRC-B)</span> <br>
    <span style="font-size: 16px; font-weight: bold;"><sup>3</sup>The Department of Camera Innovation Group, Samsung Electronics</span> <br>
    
  </p>
  <br>
  
  <figure>
    <img src="./FaceMe_files/teaser.png" alt="Teaser image" style="width: 100%;">
  </figure>
  
  <h3 style="text-align: center; font-style: italic;">
    Make the people in the photo look like you and those you are familiar with. Using a single or a few reference images,
    we can restore realistic images without any fine-tuning for identity. 
  </h3>
  <p style="text-align: center; font-size: 1.5em;">
      <a href="" target="_blank">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank">[Dataset]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank">[BibTeX]</a>
  </p>

</div>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and supports any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness. </p>
</div>
<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/background.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2 style="text-align:center;">Method</h2>
  <p>FaceMe is a fine-tuning-free personalized blind face restoration method based on diffusion model. Given a low-quality input and either single or a few high-quality references images of the same identity, FaceMe restores high-quality facial images and maintains identity consistency within seconds. </p>
  <br>
  <figure>
    <img src="./FaceMe_files/overview.png" alt="Overview image" style="width: 100%;">
  </figure>
  <p>
    For the proposed FaceMe, identity-related features from the reference image are extracted by an identity encoder, 
    simply combined to support multi-reference image inputs. We use a fixed text, 
    <i>i.e., "a photo of a face."</i>, and the combined identity-related features replace the embedding corresponding to 
    <i>"face."</i> The updated embeddings are sent to the cross-attention layer of the diffusion model to guide personalized face image restoration.
  </p>
  <p>
    To address the challenge of insufficient datasets for personalized blind face restoration, we utilize synthetic facial images as reference images to build our training data pool.
    We synthesize multiple reference facial images of the same identity as the given facial image using Arc2Face (Papantoniou et al. 2024), equipped with ControlNet (Zhang,Rao, and Agrawala 2023). 
    Given a pair of facial images (\(x_{ref} , x_{pose}\)), Arc2Face can synthesize facial images that maintain the identity of \(x_{ref}\) and the pose of \(x_{pose}\).
    We divide the FFHQ dataset into several subsets based on expressions and poses. 
    For each image synthesis, a facial image is randomly selected from each subset to serve as the pose reference \(x_{pose}\).
  </p>
</div>
<div class="content">
  <h2 style="text-align:center;">Results</h2>
  <h2>Quantitative Comparisons</h2>
  <figure>
    <img src="./FaceMe_files/Quantitative_Comparisons.png" alt="Quantitative_Comparisons" style="width: 100%;">
  </figure>
  <p>
  FaceMe achieves the best performance in PSNR, FID, LMD, and IDS, and the second-best performance in LPIPS. 
  It is worth noting that FaceMe has significantly improved in both LMD and IDS, which demonstrates its ability in personalization while minimizes the impact of ID-irrelevant features. 
  </p>
  <h2>Visual Comparisons</h2>
  <figure>
    <img src="./FaceMe_files/synth_result.png" alt="synth_result" style="width: 100%;">
  </figure>
  <figure>
    <img src="./FaceMe_files/real_result.png" alt="real_result" style="width: 100%;">
  </figure>
</div>
<div class="content">
  <h2>BibTex</h2>
  <!-- <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code>  -->
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
  </p>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
          <p>
            This website's source code is borrowed from the <a
            href="https://github.com/google/dreambooth">DreamBooth</a> project page.
          </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
